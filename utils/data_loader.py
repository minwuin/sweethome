import requests
import xml.etree.ElementTree as ET
import pandas as pd
import streamlit as st
from datetime import datetime
from dateutil.relativedelta import relativedelta
import time
import os

# ==========================================
# [ì„¤ì •] API í‚¤ ë° ê²½ë¡œ
# ==========================================
MOLIT_API_KEY = "fba6973ac6f9aed36f2b30b7dcce1fa4f6bef6c6c26cb61aff47144cc68520e5"
KAKAO_API_KEY = "5b71324d3e681cdeaa038e7725055998"

DATA_DIR = r"C:\minwoin\room\data"
if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)

# [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ì •ë¦¬ (3ê°œë¡œ í†µí•©)
ROOM_CSV_PATH = os.path.join(DATA_DIR, "room.csv")
CCTV_CSV_PATH = os.path.join(DATA_DIR, "cctv.csv")
NOISE_CSV_PATH = os.path.join(DATA_DIR, "noise.csv")          # ìˆ ì§‘ + ë…¸ë˜ë°©
CONVENIENCE_CSV_PATH = os.path.join(DATA_DIR, "convenience.csv") # í¸ì˜ì 
STORE_CSV_PATH = os.path.join(DATA_DIR, "store.csv")           # ìŒì‹ì  + ì¹´í˜

TARGET_DONGS = ["ì¡°ì˜ë™", "ëŒ€ë™", "ì„ë‹¹ë™", "ë¶€ì ë¦¬"]

# ==========================================
# 1. êµ­í† ë¶€ ì‹¤ê±°ë˜ê°€ / CCTV (ê¸°ì¡´ ìœ ì§€)
# ==========================================
def fetch_one_month_data(lawd_cd, deal_ymd):
    url = "http://apis.data.go.kr/1613000/RTMSDataSvcSHRent/getRTMSDataSvcSHRent"
    params = {"serviceKey": MOLIT_API_KEY, "LAWD_CD": lawd_cd, "DEAL_YMD": deal_ymd, "numOfRows": 1000, "pageNo": 1}
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code != 200: return []
        root = ET.fromstring(response.content)
        if root.findtext(".//resultCode") != "000": return []
        data_list = []
        for item in root.findall(".//item"):
            if item.findtext("jibun", "").strip().startswith("ì‚°"): continue
            data_list.append({
                "ë²•ì •ë™": item.findtext("umdNm", "").strip(),
                "ê±´ì¶•ë…„ë„": int(item.findtext("buildYear", "0").strip() or 0),
                "ì „ìš©ë©´ì ": float(item.findtext("totalFloorAr", 0)),
                "ë³´ì¦ê¸ˆ": int(item.findtext("deposit", "0").replace(",", "")),
                "ì›”ì„¸": int(item.findtext("monthlyRent", "0").replace(",", "")),
                "ê³„ì•½ì¼": f"{item.findtext('dealYear')}-{item.findtext('dealMonth')}-{item.findtext('dealDay')}"
            })
        return data_list
    except: return []

def get_real_estate_data(lawd_cd="47290", months=24, force_update=False):
    if os.path.exists(ROOM_CSV_PATH) and not force_update:
        try: df = pd.read_csv(ROOM_CSV_PATH, encoding='utf-8-sig')
        except: df = pd.read_csv(ROOM_CSV_PATH, encoding='cp949')
        df.columns = df.columns.str.replace('\ufeff', '').str.strip()
        # [ì¶”ê°€] ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹œì—ë„ ë³´ì¦ê¸ˆ 1000ë§Œì› ì´ˆê³¼ ë°ì´í„° ì‚­ì œ
        if 'ë³´ì¦ê¸ˆ' in df.columns:
            df = df[df['ë³´ì¦ê¸ˆ'] <= 1000]

        if 'ë²•ì •ë™' in df.columns:
            mask = df['ë²•ì •ë™'].apply(lambda x: any(target in str(x) for target in TARGET_DONGS))
            return df[mask]
    
    date_list = [ (datetime.now() - relativedelta(months=i)).strftime("%Y%m") for i in range(months) ]
    all_data = []
    for ymd in date_list:
        all_data.extend(fetch_one_month_data(lawd_cd, ymd))
        time.sleep(0.05)
    
    if not all_data: return pd.DataFrame()
    df = pd.DataFrame(all_data)
    df.columns = df.columns.str.replace('\ufeff', '').str.strip()

    # [ìˆ˜ì •] APIë¡œ ìƒˆë¡œ ë°›ì•„ì˜¨ ë°ì´í„°ì—ì„œ ë³´ì¦ê¸ˆ 1000ë§Œì› ì´ˆê³¼ ì œê±°
    # ë³´í†µ ëŒ€í•™ê°€ ì›ë£¸ ë¸”ë¡ ë¶„ì„ì„ ë°©í•´í•˜ëŠ” 'ì•„íŒŒíŠ¸'ë‚˜ 'ëŒ€í˜• ë¹Œë¼' ê±°ë˜ë¥¼ ê±°ë¥´ëŠ” ì—­í• 
    df = df[df['ë³´ì¦ê¸ˆ'] <= 1000]
    
    mask = df['ë²•ì •ë™'].apply(lambda x: any(target in str(x) for target in TARGET_DONGS))
    df_filtered = df[mask].copy()

    current_year = datetime.now().year
    df_filtered['ë…¸í›„ë„'] = df_filtered['ê±´ì¶•ë…„ë„'].apply(lambda x: current_year - x if x > 0 else 0)
    
    df_filtered.to_csv(ROOM_CSV_PATH, index=False, encoding='utf-8-sig')
    return df_filtered

def get_cctv_data():
    if not os.path.exists(CCTV_CSV_PATH): return pd.DataFrame()
    try:
        try: df = pd.read_csv(CCTV_CSV_PATH, encoding='utf-8-sig')
        except: df = pd.read_csv(CCTV_CSV_PATH, encoding='cp949')
        df.columns = df.columns.str.replace('\ufeff', '').str.strip()
        df = df.rename(columns={'WGS84ìœ„ë„': 'lat', 'WGS84ê²½ë„': 'lon', 'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'})
        if 'lat' in df.columns and 'lon' in df.columns:
            df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
            df['lon'] = pd.to_numeric(df['lon'], errors='coerce')
            return df.dropna(subset=['lat', 'lon']).drop_duplicates(subset=['lat', 'lon'])
    except: pass
    return pd.DataFrame()

# ==========================================
# [ìˆ˜ì •] ì¹´ì¹´ì˜¤ API ë‚´ë¶€ í˜¸ì¶œ í•¨ìˆ˜ (ì €ì¥ ê¸°ëŠ¥ ì œê±°, ë°ì´í„° ë¦¬í„´ë§Œ)
# ==========================================
def _fetch_api(category_code=None, keyword=None, rect_str=None):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json" if keyword else "https://dapi.kakao.com/v2/local/search/category.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"rect": rect_str, "page": 1, "size": 15}
    if category_code: params["category_group_code"] = category_code
    if keyword: params["query"] = keyword

    results = []
    while True:
        try:
            resp = requests.get(url, headers=headers, params=params, timeout=3)
            if resp.status_code != 200: break
            data = resp.json()
            for doc in data.get('documents', []):
                results.append({
                    "name": doc['place_name'],
                    "lat": float(doc['y']),
                    "lon": float(doc['x']),
                    "category": doc.get('category_name', ''),
                    "url": doc['place_url']
                })
            if not data.get('meta', {}).get('is_end'):
                params['page'] += 1
                if params['page'] > 3: break
            else: break
        except: break
        time.sleep(0.1)
    return pd.DataFrame(results)

# ==========================================
# [í•µì‹¬] 3. ê·¸ë£¹ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ë³‘í•© ì €ì¥
# ==========================================
def _get_grouped_data(save_path, fetch_funcs, min_lat, max_lat, min_lon, max_lon, force_update=False):
    # 1. íŒŒì¼ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(save_path) and not force_update:
        try:
            df = pd.read_csv(save_path, encoding='utf-8-sig')
            # í˜„ì¬ í™”ë©´ ë²”ìœ„ í•„í„°ë§
            mask = (df['lat'] >= min_lat) & (df['lat'] <= max_lat) & \
                   (df['lon'] >= min_lon) & (df['lon'] <= max_lon)
            if len(df[mask]) > 0: return df[mask]
        except: pass

    # 2. API í˜¸ì¶œ ë° ë³‘í•©
    pad = 0.01
    rect_str = f"{min_lon-pad},{min_lat-pad},{max_lon+pad},{max_lat+pad}"
    
    dfs = []
    print(f"ğŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({os.path.basename(save_path)})")
    
    for func_args, type_name in fetch_funcs:
        # func_args: {'keyword': '...'} or {'category_code': '...'}
        df = _fetch_api(rect_str=rect_str, **func_args)
        if not df.empty:
            df['type'] = type_name # êµ¬ë¶„ê°’ ì¶”ê°€ (ì˜ˆ: ìˆ ì§‘, ë…¸ë˜ë°©)
            dfs.append(df)
            
    if dfs:
        merged_df = pd.concat(dfs).drop_duplicates(subset=['lat', 'lon'])
        merged_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path} ({len(merged_df)}ê°œ)")
        
        # ë²”ìœ„ í•„í„°ë§ ë°˜í™˜
        mask = (merged_df['lat'] >= min_lat) & (merged_df['lat'] <= max_lat) & \
               (merged_df['lon'] >= min_lon) & (merged_df['lon'] <= max_lon)
        return merged_df[mask]
    
    return pd.DataFrame()

# --- ì™¸ë¶€ í˜¸ì¶œ í•¨ìˆ˜ë“¤ ---

def get_noise_data(min_lat, max_lat, min_lon, max_lon):
    """ìˆ ì§‘ + ë…¸ë˜ë°© -> noise.csv"""
    tasks = [
        ({'keyword': 'ìˆ ì§‘'}, 'ìˆ ì§‘'),
        ({'keyword': 'ë…¸ë˜ë°©'}, 'ë…¸ë˜ë°©')
    ]
    return _get_grouped_data(NOISE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_convenience_data(min_lat, max_lat, min_lon, max_lon):
    """í¸ì˜ì  -> convenience.csv"""
    tasks = [
        ({'category_code': 'CS2'}, 'í¸ì˜ì ')
    ]
    return _get_grouped_data(CONVENIENCE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_store_data(min_lat, max_lat, min_lon, max_lon):
    """ìŒì‹ì  + ì¹´í˜ -> store.csv (ìƒê°€ 1ì¸µ ì¶”ì •ìš©)"""
    tasks = [
        ({'category_code': 'FD6'}, 'ìŒì‹ì '),
        ({'category_code': 'CE7'}, 'ì¹´í˜')
    ]
    return _get_grouped_data(STORE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)