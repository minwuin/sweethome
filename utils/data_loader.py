import requests
import xml.etree.ElementTree as ET
import pandas as pd
import streamlit as st
from datetime import datetime
from dateutil.relativedelta import relativedelta
import time
import os
import numpy as np 

# ==========================================
# [ì„¤ì •] API í‚¤ ë° ê²½ë¡œ
# ==========================================
MOLIT_API_KEY = "fba6973ac6f9aed36f2b30b7dcce1fa4f6bef6c6c26cb61aff47144cc68520e5"
KAKAO_API_KEY = "5b71324d3e681cdeaa038e7725055998"
ORS_API_KEY = "eyJvcmciOiI1YjNjZTM1OTc4NTExMTAwMDFjZjYyNDgiLCJpZCI6IjQxOTY0MTQ1MTI0MDRlZGZiYWJlMWMxNTYzN2E0NDc2IiwiaCI6Im11cm11cjY0In0="

# ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
UTILS_DIR = os.path.dirname(os.path.abspath(__file__)) 
BASE_DIR = os.path.dirname(UTILS_DIR) 

# í´ë” ê²½ë¡œ ì •ì˜
DATA_DIR = os.path.join(BASE_DIR, "data")
ZICBANG_DIR = os.path.join(BASE_DIR, "zicbang")
if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)

# ê°œë³„ íŒŒì¼ ê²½ë¡œ ì •ì˜ (ì¼ì›í™”)
ZIGBANG_RAW_PATH = os.path.join(ZICBANG_DIR, "zigbang.csv")
ZIGBANG_FINAL_PATH = os.path.join(DATA_DIR, "zigbang_with_age.csv")
BUILDINGS_CSV_PATH = os.path.join(DATA_DIR, "buildings.csv")

CCTV_CSV_PATH = os.path.join(DATA_DIR, "cctv.csv")
NOISE_CSV_PATH = os.path.join(DATA_DIR, "noise.csv")
CONVENIENCE_CSV_PATH = os.path.join(DATA_DIR, "convenience.csv")
STORE_CSV_PATH = os.path.join(DATA_DIR, "store.csv")
LAMP_CSV_PATH = os.path.join(DATA_DIR, "lamp.csv")

TARGET_DONGS = ["ì¡°ì˜ë™", "ëŒ€ë™", "ì„ë‹¹ë™", "ë¶€ì ë¦¬"]

# ==========================================
# 1. êµ­í† ë¶€ ì‹¤ê±°ë˜ê°€ / CCTV (ê¸°ì¡´ ìœ ì§€)
# ==========================================

# utils/data_loader.py ì— ì¶”ê°€í•  ë‚´ìš©

def calculate_distance(lat1, lon1, lat2_arr, lon2_arr):
    R = 6371000 
    phi1, phi2 = np.radians(lat1), np.radians(lat2_arr)
    dphi = np.radians(lat2_arr - lat1)
    dlambda = np.radians(lon2_arr - lon1)
    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    return R * c

@st.cache_data
def get_ors_walking_duration(start_coords, end_coords):
    # ORS APIëŠ” [ê²½ë„, ìœ„ë„] ìˆœì„œë¥¼ ì‚¬ìš©í•¨ì— ì£¼ì˜
    url = f"https://api.openrouteservice.org/v2/directions/foot-walking"
    headers = {
        'Authorization': ORS_API_KEY,
        'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8'
    }
    params = {
        'start': f"{start_coords[1]},{start_coords[0]}",
        'end': f"{end_coords[1]},{end_coords[0]}"
    }
    
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        # ì†Œìš” ì‹œê°„(ì´ˆ) ì¶”ì¶œ í›„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
        duration_seconds = data['features'][0]['properties']['summary']['duration']
        return round(duration_seconds / 60)
    return 0

def get_realtime_zigbang_data():
    """
    1. zigbang_with_age.csv(ìµœì¢…ë³¸)ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë¦¬í„´
    2. ì—†ìœ¼ë©´ zigbang.csv(ì›ë³¸) + buildings.csv(ê±´ë¬¼)ë¥¼ í•©ì³ì„œ ìƒì„± í›„ ë¦¬í„´
    """
    # 1. ìµœì¢… íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
    if os.path.exists(ZIGBANG_FINAL_PATH):
        df = pd.read_csv(ZIGBANG_FINAL_PATH, encoding='utf-8-sig')
        return df.rename(columns={'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'}, errors='ignore')

    # 2. ìµœì¢… íŒŒì¼ì´ ì—†ì„ ê²½ìš° ë³‘í•© ì‹œì‘
    st.info("ğŸ”„ ì²˜ìŒ ì‹¤í–‰ ì‹œ ë°ì´í„° í†µí•© ì‘ì—…(ë…¸í›„ë„ ë§¤ì¹­)ì´ í•„ìš”í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    if not os.path.exists(ZIGBANG_RAW_PATH) or not os.path.exists(BUILDINGS_CSV_PATH):
        st.error(f"âŒ í•„ìˆ˜ ë°ì´í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\n- ì›ë³¸: {ZIGBANG_RAW_PATH}\n- ê±´ë¬¼: {BUILDINGS_CSV_PATH}")
        st.stop()

    df_zig = pd.read_csv(ZIGBANG_RAW_PATH)
    df_bld = pd.read_csv(BUILDINGS_CSV_PATH)

    b_lats = df_bld['lat'].values
    b_lons = df_bld['lon'].values
    b_ages = df_bld['ë…¸í›„ë„'].values

    def match_age(row):
        dists = calculate_distance(row['ìœ„ë„'], row['ê²½ë„'], b_lats, b_lons)
        min_idx = np.argmin(dists)
        return b_ages[min_idx] if dists[min_idx] <= 20 else 0

    df_zig['ë…¸í›„ë„'] = df_zig.apply(match_age, axis=1)
    
    # í†µí•© íŒŒì¼ ì €ì¥
    df_zig.to_csv(ZIGBANG_FINAL_PATH, index=False, encoding='utf-8-sig')
    st.success(f"âœ… í†µí•© ì™„ë£Œ! '{os.path.basename(ZIGBANG_FINAL_PATH)}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return df_zig.rename(columns={'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'}, errors='ignore')

def get_cctv_data():
    if not os.path.exists(CCTV_CSV_PATH): return pd.DataFrame()
    try:
        try: df = pd.read_csv(CCTV_CSV_PATH, encoding='utf-8-sig')
        except: df = pd.read_csv(CCTV_CSV_PATH, encoding='cp949')
        df.columns = df.columns.str.replace('\ufeff', '').str.strip()
        df = df.rename(columns={'WGS84ìœ„ë„': 'lat', 'WGS84ê²½ë„': 'lon', 'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'})
        if 'lat' in df.columns and 'lon' in df.columns:
            df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
            df['lon'] = pd.to_numeric(df['lon'], errors='coerce')
            return df.dropna(subset=['lat', 'lon']).drop_duplicates(subset=['lat', 'lon'])
    except: pass
    return pd.DataFrame()

def get_lamp_data():
    if not os.path.exists(LAMP_CSV_PATH): return pd.DataFrame()
    try:
        df = pd.read_csv(LAMP_CSV_PATH, encoding='utf-8-sig')
        # í•œê¸€ ì»¬ëŸ¼ëª…ì„ lat, lonìœ¼ë¡œ ë³€ê²½
        df = df.rename(columns={'ìœ„ë„': 'lat', 'ê²½ë„': 'lon'})
        if 'lat' in df.columns and 'lon' in df.columns:
            df['lat'] = pd.to_numeric(df['lat'], errors='coerce')
            df['lon'] = pd.to_numeric(df['lon'], errors='coerce')
            return df.dropna(subset=['lat', 'lon'])
    except: pass
    return pd.DataFrame()

# ==========================================
# [ìˆ˜ì •] ì¹´ì¹´ì˜¤ API ë‚´ë¶€ í˜¸ì¶œ í•¨ìˆ˜ (ì €ì¥ ê¸°ëŠ¥ ì œê±°, ë°ì´í„° ë¦¬í„´ë§Œ)
# ==========================================
def _fetch_api(category_code=None, keyword=None, rect_str=None):
    url = "https://dapi.kakao.com/v2/local/search/keyword.json" if keyword else "https://dapi.kakao.com/v2/local/search/category.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"rect": rect_str, "page": 1, "size": 15}
    if category_code: params["category_group_code"] = category_code
    if keyword: params["query"] = keyword

    results = []
    while True:
        try:
            resp = requests.get(url, headers=headers, params=params, timeout=3)
            if resp.status_code != 200: break
            data = resp.json()
            for doc in data.get('documents', []):
                results.append({
                    "name": doc['place_name'],
                    "lat": float(doc['y']),
                    "lon": float(doc['x']),
                    "category": doc.get('category_name', ''),
                    "url": doc['place_url']
                })
            if not data.get('meta', {}).get('is_end'):
                params['page'] += 1
                if params['page'] > 3: break
            else: break
        except: break
        time.sleep(0.1)
    return pd.DataFrame(results)

# ==========================================
# [í•µì‹¬] 3. ê·¸ë£¹ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ë³‘í•© ì €ì¥
# ==========================================
def _get_grouped_data(save_path, fetch_funcs, min_lat, max_lat, min_lon, max_lon, force_update=False):
    # 1. íŒŒì¼ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(save_path) and not force_update:
        try:
            df = pd.read_csv(save_path, encoding='utf-8-sig')
            # í˜„ì¬ í™”ë©´ ë²”ìœ„ í•„í„°ë§
            mask = (df['lat'] >= min_lat) & (df['lat'] <= max_lat) & \
                   (df['lon'] >= min_lon) & (df['lon'] <= max_lon)
            if len(df[mask]) > 0: return df[mask]
        except: pass

    # 2. API í˜¸ì¶œ ë° ë³‘í•©
    pad = 0.01
    rect_str = f"{min_lon-pad},{min_lat-pad},{max_lon+pad},{max_lat+pad}"
    
    dfs = []
    print(f"ğŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({os.path.basename(save_path)})")
    
    for func_args, type_name in fetch_funcs:
        # func_args: {'keyword': '...'} or {'category_code': '...'}
        df = _fetch_api(rect_str=rect_str, **func_args)
        if not df.empty:
            df['type'] = type_name # êµ¬ë¶„ê°’ ì¶”ê°€ (ì˜ˆ: ìˆ ì§‘, ë…¸ë˜ë°©)
            dfs.append(df)
            
    if dfs:
        merged_df = pd.concat(dfs).drop_duplicates(subset=['lat', 'lon'])
        merged_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path} ({len(merged_df)}ê°œ)")
        
        # ë²”ìœ„ í•„í„°ë§ ë°˜í™˜
        mask = (merged_df['lat'] >= min_lat) & (merged_df['lat'] <= max_lat) & \
               (merged_df['lon'] >= min_lon) & (merged_df['lon'] <= max_lon)
        return merged_df[mask]
    
    return pd.DataFrame()

# --- ì™¸ë¶€ í˜¸ì¶œ í•¨ìˆ˜ë“¤ ---

def get_noise_data(min_lat, max_lat, min_lon, max_lon):
    """ìˆ ì§‘ + ë…¸ë˜ë°© -> noise.csv"""
    tasks = [
        ({'keyword': 'ìˆ ì§‘'}, 'ìˆ ì§‘'),
        ({'keyword': 'ë…¸ë˜ë°©'}, 'ë…¸ë˜ë°©')
    ]
    return _get_grouped_data(NOISE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_convenience_data(min_lat, max_lat, min_lon, max_lon):
    """í¸ì˜ì  -> convenience.csv"""
    tasks = [
        ({'category_code': 'CS2'}, 'í¸ì˜ì ')
    ]
    return _get_grouped_data(CONVENIENCE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)

def get_store_data(min_lat, max_lat, min_lon, max_lon):
    """ìŒì‹ì  + ì¹´í˜ -> store.csv (ìƒê°€ 1ì¸µ ì¶”ì •ìš©)"""
    tasks = [
        ({'category_code': 'FD6'}, 'ìŒì‹ì '),
        ({'category_code': 'CE7'}, 'ì¹´í˜')
    ]
    return _get_grouped_data(STORE_CSV_PATH, tasks, min_lat, max_lat, min_lon, max_lon)